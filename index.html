<!DOCTYPE html>
<!-- saved from url=(0034)https://deqings.github.io/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Deqing Sun</title>
<meta content="Deqing Sun" name="Deqing Sun">
<link href="./Deqing_Sun_files/style.css" rel="stylesheet" type="text/css">
<script src="./Deqing_Sun_files/jquery-1.11.1.min.js" type="text/javascript"></script>  
</head>

<body>
  <div class="menu"> <a href="https://deqings.github.io/index.html">Home</a> <a href="https://deqings.github.io/index.html#service">Services</a> <a href="https://deqings.github.io/index.html#publications">Publications</a> 
<a href="https://deqings.github.io/index.html#awards"> Awards/Honors</a>   
<a href="https://deqings.github.io/index.html#students"> Students</a>     
  <!-- <a href="https://deqings.github.io/index.html#code"> Code/Data</a>  -->
  </div>
  <div class="container">
    <table border="0">
      <tbody><tr>
        <td><img src="./Deqing_Sun_files/profile_crop.jpeg" width="200"></td>
        <td style="width: 10px">&nbsp;</td>
        <td valign="top" width="500">
          <span class="name">Deqing Sun</span>
          <p class="information"><br>
           Staff Research Scientist and Manager, <a href="https://research.google/locations/cambridge/">Google</a></p>
          <p class="information">Google Inc.<br>
            355 Main Street<br>
          Cambridge, MA 02142</p>
          <p class="information"><strong>Email</strong>: <span class="unselectable">deqing<span class="mock">sun</span><span class="hide">xkxkxk</span>@google.com</span></p>
        </td>
      </tr>
    </tbody></table>
    
    <a id="service" class="anchor"></a><span class="section">Services</span>
    <p class="service">Area Chairs for 
            <a href="https://eccv2022.ecva.net/">ECCV 2022</a>,
            <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a>,
            <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>, 
            <a href="https://www.bmvc2020-conference.com/people/area-chairs/">BMVC 2020</a>,                                     
            <a href="http://cvpr2019.thecvf.com/">CVPR 2019</a>, 
            <a href="https://bmvc2019.org/">BMVC 2019</a>,             
            <a href="https://eccv2018.org/">ECCV 2018</a>                </p>
    <p class="service">
      Co-organized Workshop on  <a > What is Motion for? </a> at ECCV 2022
    </p>                
    <p class="service">
      Co-organized Workshop/Tutorial on <a href="http://visual.cs.brown.edu/workshops/aicc2021"> AI for Content Creation </a> at CVPR 2019-2022 and SIGGRAPH 2018
    </p>
    <p class="service">
      Co-organized Workshop on  <a href="https://www.pure.ed.ac.uk/ws/portalfiles/portal/84961059/What_Is_Optical_Flow_GUNEY_DoA190318_AFV.pdf"> What is Optical Flow for? </a> at ECCV 2018
    </p>    
    <p class="service">
      Guest editor for special issue on  <a href="https://www.sciencedirect.com/journal/computer-vision-and-image-understanding/special-issue/1099DHQ0PLZ"> Deep Learning for Image Restoration </a> in Computer Vision and Image Understanding
    </p>    
    <p class="service">
      Reviewer for  <a href="https://www.nsf.gov/">  National Science Foundation (NSF)
    </p>         
    <p>&nbsp;</p>

    <!-- Publication session -->
    <a id="publications" class="anchor"></a><span class="section">Publications <a href="https://scholar.google.com/citations?user=t4rgICIAAAAJ&hl=en&oi=sra"> Google scholar profile </a> </span>
    <table border="0" width="90%" class="paper">
      <tbody>



      <tr>
        <td>
          <img src="./images/pubpic/arXiv22_What.png" class="PaperThumbnail" width="120">
        </td>
        <td>
        Deqing Sun, Charles Herrmann, Fitsum Reda, Michael Rubinstein, David Fleet, William T Freeman
  <strong>What Makes RAFT Better Than PWC-Net?
</strong> <em>arXiv 2022</em>.
          [<a href="https://arxiv.org/pdf/2203.10712.pdf"><font color="#000080">pdf</font></a>]
          [<a>code</font></a>]
        </td>
      </tr>     

      <tr>
        <td>
          <img src="./images/pubpic/arXiv22_FILM.gif" class="PaperThumbnail" width="120">
        </td>
        <td bgcolor="#e9eaed">Fitsum Reda, Janne Kontkanen, Eric Tabellion, Deqing Sun, Caroline Pantofaru, Brian Curless 
          <strong>FILM: Frame Interpolation for Large Motion</strong>. <em>arXiv 2022</em>.
          [<a href="https://arxiv.org/pdf/2202.04901.pdf"><font color="#000080">pdf</font></a>]
          [<a href="https://github.com/google-research/frame-interpolation"><font color="#000080">code</font></a>]
        </td>
      </tr>      

      <tr>
        <td>
          <img src="./images/pubpic/CVPR22_PyramidAT2.png" class="PaperThumbnail" width="120">
        </td>
        <td>
        Charles Herrmann*, Kyle Sargent*, Lu Jiang, Ramin Zabih, Huiwen Chang, Ce Liu, Dilip Krishnan, Deqing Sun (*equal contribution)
  <strong>Pyramid Adversarial Training Improves ViT Performance</strong>. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (CVPR) 2022. <span class="oral">Oral presentation.</span>  
          [<a href="https://arxiv.org/pdf/2111.15121.pdf"><font color="#000080">pdf</font></a>]
          [<a>code</font></a>]
        </td>
      </tr>     

      <tr>
        <td>
          <img src="./images/pubpic/CVPR22_Kubric.png" class="PaperThumbnail" width="120">
        </td>
        <td bgcolor="#e9eaed">
          Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji, Hsueh-Ti (Derek) Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong, Andrea Tagliasacchi <strong>Kubric: A scalable dataset generator  </strong>. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (CVPR) 2022.  
          [<a href="https://arxiv.org/pdf/2203.03570.pdf"><font color="#000080">pdf</font></a>]
          [<a href="https://github.com/google-research/kubric"><font color="#000080">code</font></a>]
        </td>
      </tr>        


    <tr>
      <td>
          <img src="./images/pubpic/cvpr18_dualcnn.png" class="PaperThumbnail" width="120">
        </td>
        <td >
          Jinshan Pan, Deqing Sun, Jiawei Zhang, Jinhui Tang, Jian Yang, Yu-Wing Tai, Ming-Hsuan Yang  <strong>Dual Convolutional Neural Networks for Low-Level Vision</strong>. <em>International Journal of Computer Vision</em>  (IJCV), 2022.  
          [<a href="https://link.springer.com/article/10.1007/s11263-022-01583-y">pdf</font></a>]
          [<a href="https://github.com/jspan/dualcnn"><font color="#000080">code</font></a>]
        </td>
      </tr>   


     <tr>
        <td>
          <img src="./images/pubpic/SIGGRAPH22_FaceUnblur.jpeg" class="PaperThumbnail" width="120">
        </td>
        <td bgcolor="#e9eaed">
           Wei-Sheng Lai, YiChang Shih, Lun-Cheng Chu, Xiaotong Wu, Sung-fang Tsai, Michael Krainin, Deqing Sun, Chia-Kai Liang <strong>Face Deblurring using Dual Camera Fusion on Mobile Phones   </strong>. <em>ACM Transactions on Graphics (Proceedings of SIGGRAPH 2022)</em>.  
          <!-- [<a href=""><font color="#000080">pdf</font></a>] -->
          [<a href="https://www.androidpolice.com/the-pixel-6s-camera-can-automatically-sharpen-blurry-faces/"><font color="#000080">Pixel 6 Launch</font></a>]
        </td>
      </tr>        




        <tr>
        <td>
          <img src="./images/pubpic/ICLR22_ViDT.png" class="PaperThumbnail" width="120">
        </td>
        <td>
          H. Song, D. Sun, S. Chun, V. Jampani, D. Han, B. Heo, W. Kim, and M-H Yang .  <strong>ViDT: An Efficient and Effective Fully Transformer-based Object Detector</strong>. <em>International Conference on Learning Representations </em> (ICLR) 2022. 
          [<a href="https://arxiv.org/pdf/2110.03921.pdf"><font color="#000080">pdf</font></a>]
          [<a href="https://github.com/naver-ai/vidt"><font color="#000080">code</font></a>]
        </td>
      </tr>   


      <tr>
        <td>
          <img src="./images/pubpic/rbreakdance-flare-all.gif" class="PaperThumbnail" width="120">
        </td>
        <td bgcolor="#e9eaed">
          G. Yang, D. Sun, V. Jampani, D. Vlasic, F. Cole, C. Liu and D. Ramanan. <strong>ViSER: Video-Specific Surface Embeddings for Articulated 3D Shape Reconstruction</strong>. <em>Neural Information Processing Systems</em> (NeurIPS) 2021.  <span class="oral">Spotlight presentation.</span>  
          [<a href="https://www.contrib.andrew.cmu.edu/~gengshay/ViSER.pdf"><font color="#000080">pdf</font></a>]
          [<a href="https://viser-shape.github.io/"><font color="#000080">webpage</font></a>]
          [<a href="https://github.com/gengshan-y/viser-release"><font color="#000080">code</font></a>]
        </td>
      </tr>   

        <tr>
        <td>
          <img src="./images/pubpic/CVPR21_AutoFlow.jpg" class="PaperThumbnail" width="120">
        </td>
        <td>
          D. Sun, D. Vlasic, C. Herrmann, V. Jampani, M. Krainin, H. Chang, R. Zabih, W.T. Freeman and C. Liu.  <strong>AutoFlow: Learning a Better Training Set for Optical Flow</strong>. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (CVPR) 2021. <span class="oral">Oral presentation.</span>  
          [<a href="https://arxiv.org/abs/2104.14544"><font color="#000080">pdf</font></a>]
          [<a href="https://autoflow-google.github.io/"><font color="#000080">webpage</font></a>]
          [<a href="https://autoflow-google.github.io/#data"><font color="#000080">data</font></a>]
        </td>
      </tr>                    

      <tr>
        <td>
          <img src="./images/pubpic/CVPR21_LASR.jpg" class="PaperThumbnail" width="120">
        </td>
        <td bgcolor="#e9eaed">
          G. Yang, D. Sun, V. Jampani, D. Vlasic, F. Cole, H. Chang, D. Ramanan, W.T. Freeman and C. Liu. <strong>LASR: Learning Articulated Shape Reconstruction from a Monocular Video</strong>. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (CVPR) 2021.  
          [<a href="https://arxiv.org/pdf/2105.02976.pdf"><font color="#000080">pdf</font></a>]
          [<a href="https://lasr-google.github.io/"><font color="#000080">webpage</font></a>]
          [<a href="https://github.com/google/lasr"><font color="#000080">code</font></a>]
        </td>
      </tr>                    

      <tr>
        <td>
          <img src="./images/pubpic/CVPR21_humangps.jpeg" class="PaperThumbnail" width="120">
        </td>
        <td>
          F. Tan, D. Tang, M. Dou, K. Guo, R. Pandey, C. Keskin, R. Du, D. Sun, S. Bouaziz, S. Fanello, P. Tan and Y. Zhang. <strong>HumanGPS: Geodesic PreServing Feature for Dense Human Correspondences</strong>. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (CVPR) 2021.  
          [<a href="https://arxiv.org/abs/2103.15573"><font color="#000080">pdf</font></a>]
          [<a href="https://feitongt.github.io/HumanGPS/"><font color="#000080">webpage</font></a>]
          [<a href="https://github.com/googleinterns/humangps"><font color="#000080">code</font></a>]
        </td>
      </tr>      

      <tr>
        <td>
          <img src="./images/pubpic/ASGNet_CVPR21_teaser.png" class="PaperThumbnail" width="120">
        </td>
        <td  bgcolor="#e9eaed">
          G. Li, V. Jampani, L. Sevilla-Lara, D.Sun, J. Kim and J. Kim. <strong>Adaptive Prototype Learning and Allocation for Few-Shot Segmentation
</strong>. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (CVPR) 2021.  
          [<a href="https://arxiv.org/pdf/2104.01893.pdf"><font color="#000080">pdf</font></a>]
          [<a href="https://laurasevilla.me/cvpr-21/"><font color="#000080">webpage</font></a>]
          [<a href="https://github.com/Reagan1311/ASGNet"><font color="#000080">code</font></a>]
        </td>
      </tr>      



    <tr>
        <td>
          <img src="./images/pubpic/ECCV2020_LCV.png" class="PaperThumbnail" width="120">
        </td>
        <td>
          T. Xiao, J. Yuan, D. Sun, Q. Wang, X. Zhang, K. Xu and M-H. Yang. <strong>Learnable Cost Volume Using the Cayley
Representation </strong>. <em>European Conference on Computer Vision</em> (ECCV) 2020.  
          [<a href="https://arxiv.org/pdf/2007.11431.pdf"><font color="#000080">pdf</font></a>]
          [<a href="https://github.com/Prinsphield/LCV"><font color="#000080">code</font></a>]
        </td>
      </tr>    

     
      
    <tr>
      <td>
          <img src="./images/pubpic/TPAMI2020.png" class="PaperThumbnail" width="120">
        </td>
        <td bgcolor="#e9eaed">
          D. Sun, X. Yang, M-Y. Liu and J. Kautz.  <strong>  Models Matter, So Does Training: An Empirical Study of CNNs for Optical Flow Estimation
 </strong>. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence </em>(TPAMI), pp. 1408-1423, Vol. 42, Issue 6, 2020</span>
          [<a href="https://arxiv.org/pdf/1809.05571.pdf">pdf</font></a>]
          [<a href="https://github.com/NVlabs/PWC-Net"><font color="#000080">code</font></a>]
        </td>
      </tr>    


     <tr>
        <td>
          <img src="./images/pubpic/sense_teaser.png" class="PaperThumbnail" width="120">
        </td>
        <td >
          H. Jiang, D. Sun, V. Jampani, Z. Lv, E. Learned-Miller and J. Kautz. <strong>SENSE: A Shared Encoder Network for Scene Flow Estimation</strong>. <em>IEEE/CVF Conference on Computer Vision</em> (ICCV) 2019.  <span class="oral">Oral presentation.</span>  
          [<a href="https://arxiv.org/pdf/1910.12361.pdf"><font color="#000080">pdf</font></a>]
          [<a href="https://github.com/NVlabs/SENSE"><font color="#000080">code</font></a>]
        </td>
      </tr>    



     <tr>
        <td>
          <img src="./images/pubpic/ICCV2019-cycle.png" class="PaperThumbnail" width="120">
        </td>
        <td bgcolor="#e9eaed">
          F Reda, D Sun, A Dundar, M Shoeybi, G Liu, K. Shih, A. Tao, J. Kautz and B. Catanzaro. <strong>Unsupervised Video Interpolation Using Cycle Consistency</strong>. <em>IEEE/CVF Conference on Computer Vision</em> (ICCV) 2019.  
          [<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Reda_Unsupervised_Video_Interpolation_Using_Cycle_Consistency_ICCV_2019_paper.pdf"><font color="#000080">pdf</font></a>]
          [<a href="https://github.com/NVIDIA/unsupervised-video-interpolation"><font color="#000080">code</font></a>]
        </td>
      </tr>    


     <tr>
        <td>
          <img src="./images/pubpic/BMVC2019-stitching.png" class="PaperThumbnail" width="120">
        </td>
        <td>
          W-S. Lai, O. Gallo, J. Gu, D. Sun, M-H. Yang, and J. Kautz. <strong>Video Stitching for Linear Camera Arrays</strong>. <em>British Machine Vision Conference</em> (BMVC) 2019.  
          [<a href="https://arxiv.org/pdf/1907.13622.pdf"><font color="#000080">pdf</font></a>]
        </td>
      </tr>    



      <tr>
        <td>
          <img src="./images/pubpic/cc_2019_teaser.png" class="PaperThumbnail" width="120">
        </td>
        <td bgcolor="#e9eaed">
          A. Ranjan, V. Jampani, L. Balles, K. Kim, D. Sun, J. Wulff and M. J. Black. <strong>Competitive Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation</strong>. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (CVPR) 2019.  
          [<a href="https://arxiv.org/pdf/1805.09806.pdf"><font color="#000080">pdf</font></a>]
          [<a href="http://github.com/anuragranj/cc"><font color="#000080">code</font></a>]
        </td>
      </tr>   


      <tr>
        <td>
          <img src="./images/pubpic/pacnet_2019_teaser.png" class="PaperThumbnail" width="120">
        </td>
        <td>
          H. Su, V. Jampani, D. Sun, O. Gallo, E. Learned-Miller and J. Kautz. <strong>Pixel-Adaptive Convolutional Neural Networks</strong>. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (CVPR) 2019.  
          [<a href="https://arxiv.org/pdf/1904.05373.pdf"><font color="#000080">pdf</font></a>]
          [<a href="https://suhangpro.github.io/pac/"><font color="#000080">webpage</font></a>]          
          [<a href="https://github.com/NVlabs/pacnet"><font color="#000080">code</font></a>]
        </td>
      </tr>    

 



    <tr>
        <td>
          <img src="./images/pubpic/WACV2019.png" class="PaperThumbnail" width="120">
        </td>
        <td bgcolor="#e9eaed">
          Z. Ren, O. Gallo, D. Sun, M-H Yang, E. B. Sudderth and J. Kautz. <strong>A Fusion Approach for Multi-Frame Optical Flow Estimation</strong>. <em>IEEE Winter Conference on Applications of Computer Vision </em> (WACV) 2019.  
          [<a href="https://arxiv.org/pdf/1810.10066.pdf"><font color="#000080">pdf</font></a>]
          [<a href="https://github.com/NVlabs/PWC-Net/tree/master/Multi_Frame_Flow"><font color="#000080">code</font></a>]
        </td>
      </tr>    


    <tr>
        <td>
          <img src="./images/pubpic/ssn_sample.jpeg" class="PaperThumbnail" width="120">
        </td>
        <td>
          V. Jampani, D. Sun, M-Y. Liu, M-H. Yang and J. Kautz. <strong>Superpixel Sampling Networks</strong>. <em>European Conference on Computer Vision</em> (ECCV) 2018.  
          [<a href="https://varunjampani.github.io/papers/jampani18_SSN.pdf"><font color="#000080">pdf</font></a>]
          [<a href="http://varunjampani.github.io/ssn"><font color="#000080">webpage</font></a>]          
          [<a href="https://github.com/NVlabs/ssn_superpixels"><font color="#000080">code</font></a>]
        </td>
      </tr>    




    <tr>
        <td>
          <img src="./images/pubpic/ECCV2018-DoF.png" class="PaperThumbnail" width="120">
        </td>
        <td bgcolor="#e9eaed">
          X. Xu, D. Sun, S. Liu, W. Ren, Y. Zhang, M-H. Yang and J. Sun. <strong>Rendering Portraitures from Monocular Camera and Beyond </strong>. <em>European Conference on Computer Vision</em> (ECCV) 2018.  
          [<a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Xiangyu_Xu_Rendering_Portraitures_from_ECCV_2018_paper.pdf"><font color="#000080">pdf</font></a>]
        </td>
      </tr>    

    <tr>
        <td>
          <img src="./images/pubpic/ECCV2018-dataterm.png" class="PaperThumbnail" width="120">
        </td>
        <td>
          J. Dong, J. Pan, D. Sun, Z. Su and MH Yang. <strong>Learning Data Terms for Non-blind Deblurring </strong>. <em>European Conference on Computer Vision</em> (ECCV) 2018.  
          [<a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Jiangxin_Dong_Learning_Data_Terms_ECCV_2018_paper.pdf"><font color="#000080">pdf</font></a>]
        </td>
      </tr>    



      <tr>
        <td>
          <img src="./images/pubpic/ECCV2018-RGBD.png" class="PaperThumbnail" width="120">
        </td>
        <td bgcolor="#e9eaed">
          Z. Lv, K. Kim, A. Troccoli, A. Sun, J. M. Rehg and Jan Kautz. <strong>
          Learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation</strong>. <em>European Conference on Computer Vision</em> (ECCV) 2018.  
          [<a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Zhaoyang_Lv_Learning_Rigidity_in_ECCV_2018_paper.pdf"><font color="#000080">pdf</font></a>]
          [<a href="https://github.com/NVlabs/learningrigidity"><font color="#000080">code </font></a>]
          [<a href="https://github.com/lvzhaoyang/RefRESH"><font color="#000080">data </font></a>]          
        </td>
      </tr>  


    <tr>
      <td>
          <img src="./images/pubpic/pwc_net.png" class="PaperThumbnail" width="120">
        </td>
        <td >
          D. Sun, X. Yang, M-Y. Liu and J. Kautz.  <strong>PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume</strong>. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (CVPR) 2018. <span class="oral">Oral presentation. </span>  
          <span class="award">Winner of optical flow competition.</span>  <span class="award">NVAIL Pioneering Research Award.</span>  
          [<a href="https://arxiv.org/pdf/1709.02371.pdf"><font color="#000080">pdf</font></a>]
          [<a href="https://youtu.be/LBJ20kxr1a0?t=421"><font color="#000080">CVPR talk</font></a>]
          [<a href="https://github.com/NVlabs/PWC-Net"><font color="#000080">code</font></a>]
        </td>
      </tr>     



 

    <tr>
      <td>
          <img src="./images/pubpic/slomo_teaser.jpeg" class="PaperThumbnail" width="120">
        </td>
        <td bgcolor="#e9eaed">
          H. Jiang, D. Sun, V. Jampani, M-H. Yang, E. Learned-Miller and J. Kautz.  <strong>Super SloMo: High Quality Estimation of Multiple Intermediate Frames for Video Interpolation</strong>. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (CVPR) 2018. <span class="oral">Spotlight presentation. </span>  <span class="oral">Incorporated into NVIDIA NGX SDK for the Turing GPU. </span> 
          [<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Jiang_Super_SloMo_High_CVPR_2018_paper.pdf"><font color="#000080">pdf</font></a>]
          [<a href="https://www.youtube.com/watch?v=rkSzRbM4lIM"><font color="#000080">CVPR talk</font></a>]
          [<a href="https://www.youtube.com/watch?v=MjViy6kyiqs"><font color="#000080">video results</font></a>]          
          [<a href="https://gizmodo.com/nvidia-is-using-ai-to-perfectly-fake-slo-mo-videos-1826923905"><font color="#000080">news</font></a>]
          [<a href="https://github.com/NVIDIA/unsupervised-video-interpolation"><font color="#000080">code</font></a>]
        </td>
      </tr>  


    <tr>
      <td>
          <img src="./images/pubpic/splatnet_teaser.png" class="PaperThumbnail" width="120">
        </td>
        <td>
          H. Su, V. Jampani, D. Sun, S. Maji, E. Kalogerakis, M-H. Yang and J. Kautz.  <strong>SPLATNet: Sparse Lattice Networks for Point Cloud Processing</strong>. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (CVPR) 2018. 
          <span class="oral">Oral presentation.</span>  </span>   <span class="award">Best Paper Honorable Mention.</span>  
          <span class="award">NVAIL Pioneering Research Award.</span> 
          [<a href="https://arxiv.org/abs/1802.08275.pdf"><font color="#000080">pdf</font></a>]
          [<a href="https://youtu.be/x18WUuBNK7E?t=332"><font color="#000080">CVPR talk</font></a>]
          [<a href="https://github.com/NVlabs/splatnet"><font color="#000080">code</font></a>]
        </td>
      </tr>    

   <tr>
      <td>
          <img src="./images/pubpic/cvpr18_dualcnn.png" class="PaperThumbnail" width="120">
        </td>
        <td bgcolor="#e9eaed">
          J. Pan, S. Liu, D. Sun, J. Zhang, Y. Liu, J. Ren, Z. Li, J. Tang, H. Lu, Y-W Tai and M-H Yang.  <strong>Learning Dual Convolutional Neural Networks for Low-Level Vision</strong>. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (CVPR) 2018. 

          [<a href="https://faculty.ucmerced.edu/mhyang/papers/cvpr2018_dual_cnn.pdf.pdf"><font color="#000080">pdf</font></a>]
          [<a href="https://sites.google.com/site/jspanhomepage/dualcnn">webpage</font></a>]
          [<a href="https://github.com/jspan/dualcnn"><font color="#000080">code</font></a>]
        </td>
      </tr>    

   <tr>
      <td>
          <img src="./images/pubpic/seal_teaser2.png" class="PaperThumbnail" width="120">
        </td>
        <td >
          Y-H. Tsai, M-Y. Liu, D. Sun, M-H. Yang, and J. Kautz.  <strong>Learning Superpixels with Segmentation-Aware Affinity Loss </strong>. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (CVPR) 2018. 

          [<a href="https://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0102.pdf"><font color="#000080">pdf</font></a>]
          [<a href="https://sites.google.com/site/wctu1009/cvpr18_superpixel">webpage</font></a>]
          [<a href="https://github.com/wctu/SEAL"><font color="#000080">code</font></a>]
        </td>
      </tr>  

    <tr>
      <td>
          <img src="./images/pubpic/AAAI2018.png" class="PaperThumbnail" width="120">
        </td>
        <td bgcolor="#e9eaed">
          W-C. Tu, M-Y. Liu, V. Jampani, D. Sun, S-Y. Chien, M-H. Yang and J. Kautz.  <strong>Learning Binary Residual Representations for Domain-specific Video Streaming </strong>. <em> AAAI conference on artificial intelligence</em> (AAAI) 2018. 

          [<a href="https://arxiv.org/pdf/1712.05087.pdf"><font color="#000080">pdf</font></a>]
        </td>
      </tr>    




    <tr>
        <td>
          <img src="./images/pubpic/2017-3DV.png" class="PaperThumbnail" width="120">
        </td>
        <td>
          Z. Ren, D. Sun,  J. Kautz, E. B. Sudderth. <strong>Cascaded Scene Flow Prediction using Semantic Segmentation</strong>. <em>IEEE International Conference on 3D Vision </em> (3DV) 2017.  
          [<a href="https://arxiv.org/pdf/1707.08313.pdf"><font color="#000080">pdf</font></a>]
        </td>
      </tr>    

    <tr>
      <td>
          <img src="./images/pubpic/tpami17.png" class="PaperThumbnail" width="120">
        </td>
        <td bgcolor="#e9eaed">
          J. Pan, D. Sun, H. Pfister and M-H. Yang.  <strong>  Deblurring images via dark channel prior</strong>. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence </em>(TPAMI), pp. 2315-2328, Vol. 40, Issue 10, 2017</span>
          [<a href="http://faculty.ucmerced.edu/mhyang/papers/pami18_dark_channel.pdf">pdf</font></a>]
          [<a href="https://pan.baidu.com/s/1DdHdq8-vOHYBlPc7JLN1fw"><font color="#000080">code</font></a>]
        </td>
      </tr>    

    <tr>
      <td>
          <img src="./images/pubpic/iccv17.gif" class="PaperThumbnail" width="120">
        </td>
        <td >
          X. Xu, D. Sun, J. Pan, Y. Zhang, H. Pfister and M-H. Yang.  <strong>Learning to Super-Resolve Blurry Face and Text Images</strong>. <em>IEEE/CVF  Conference on Computer Vision </em> (ICCV) 2017. 
          [<a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Xu_Learning_to_Super-Resolve_ICCV_2017_paper.pdf">pdf</font></a>]
          [<a href="https://www.google.com/url?q=https%3A%2F%2Fwww.dropbox.com%2Fs%2Fybwsrf92k45v1q7%2Fcode_release_Joint_iccv17.zip%3Fdl%3D0&sa=D&sntz=1&usg=AFQjCNFJaqOIJFG5wnIKrJMEmY4RzwyyEQ"><font color="#000080">code</font></a>]
        </td>
      </tr>    

    <tr>
      <td>
          <img src="./images/pubpic/teaser_lightfield.jpeg" class="PaperThumbnail" width="120">
        </td>
        <td bgcolor="#e9eaed">
          N. Bonneel,  J. Tompkin,  D. Sun, O. Wang, K. Sunkavalli, S. Paris and H. Pfister.  <strong> Consistent Video Filtering for Camera Arrays</strong>. <em>Computer Graphics Forum (Proceedings of Eurographics 2017)</em>. 
          [<a href="https://perso.liris.cnrs.fr/nicolas.bonneel/cameraarrays/EG2017___Consistency_for_Camera_Arrays.pdf">pdf</font></a>]
          [<a href="https://perso.liris.cnrs.fr/nicolas.bonneel/cameraarrays/"><font color="#000080">code</webpage></a>]
        </td>
      </tr>   

    <tr>
      <td>
          <img src="./images/pubpic/deblur_dark_channel.png" class="PaperThumbnail" width="120">
        </td>
        <td>
          J. Pan, D. Sun, H. Pfister and M-H. Yang.  <strong>Blind Image Deblurring Using Dark Channel Prior</strong>. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (CVPR) 2016. <span class="oral">Oral presentation. </span>  
          [<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Pan_Blind_Image_Deblurring_CVPR_2016_paper.pdf">pdf</font></a>]
          [<a href="https://pan.baidu.com/s/1DdHdq8-vOHYBlPc7JLN1fw"><font color="#000080">code</font></a>]
        </td>
      </tr>    

    <tr>
      <td>
          <img src="./images/pubpic/flow_cvpr16_teaser.png" class="PaperThumbnail" width="120">
        </td>
        <td bgcolor="#e9eaed">
          L. Sevilla, D. Sun, V. Jampani and  M. J. Black.  <strong>Optical Flow with Semantic Segmentation and Localized Layers</strong>. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (CVPR) 2016. <span class="oral">Spotlight presentation. </span>  
          [<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Sevilla-Lara_Optical_Flow_With_CVPR_2016_paper.pdf">pdf</font></a>]
          [<a href="https://ps.is.tuebingen.mpg.de/research_projects/semantic-optical-flow"><font color="#000080">webpage</font></a>]          
          [<a href="https://youtu.be/QwmBSTWgr_s"><font color="#000080">video</font></a>]          
          [<a href="https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/281/semantic_flow_code_release.zip"><font color="#000080">code</font></a>]
        </td>
      </tr>   


    <tr>
      <td>
          <img src="./images/pubpic/teaser_consistency.jpeg" class="PaperThumbnail" width="120">
        </td>
        <td >
          N. Bonneel,  J. Tompkin, K. Sunkavalli, D. Sun,  S. Paris and H. Pfister.  <strong> Blind Video Temporal Consistency</strong>. <em>ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2015)</em>. 
          [<a href="https://perso.liris.cnrs.fr/nicolas.bonneel/consistency/temporalconsistency_low.pdf">pdf</font></a>]
          [<a href="https://perso.liris.cnrs.fr/nicolas.bonneel/consistency/"><font color="#000080">code</webpage></a>]
          [<a href="https://github.com/nbonneel/blindconsistency"><font color="#000080">code</font></a>]
        </td>
      </tr>   


      <tr>
        <td>
          <img src="./images/pubpic/CVPR2015-RGBD.png" width="120">
        </td>
        <td bgcolor="#e9eaed">
          D. Sun, E. B. Sudderth and H. Pfister.  <strong>Layered RGBD Scene Flow Estimation</strong>. <font face="Times New Roman"><em>IEEE Conference on Computer Vision and Pattern Recognition </em>(CVPR), 2015. 
          [<a href="https://openaccess.thecvf.com/content_cvpr_2015/papers/Sun_Layered_RGBD_Scene_2015_CVPR_paper.pdf"><font color="#000080">pdf</font></a>]
        </td>
      </tr>

    <tr>
      <td>
          <img src="./images/pubpic/ijcv2014.png" class="PaperThumbnail" width="120">
        </td>
        <td >
          D. Sun, S. Roth and M. J. Black.  <strong>A Quantitative Analysis of Current Practices in Optical Flow Estimation and the Principles Behind Them</strong>. <em>International Journal of Computer Vision</em>  (IJCV), 106(2):115-137, 2014. <span class="award">Journal version of the Longuet-Higgins Prize paper on "Secrets of Optical Flow". </span>  
          [<a href="https://link.springer.com/content/pdf/10.1007/s11263-013-0644-x.pdf">pdf</font></a>]
          [<a href="public_files/ijcv_flow_code.zip"><font color="#000080">code</font></a>]
        </td>
      </tr>   

    <tr>
      <td>
          <img src="./images/pubpic/teaser_intrinsic.jpeg" class="PaperThumbnail" width="120">
        </td>
        <td bgcolor="#e9eaed">
          N. Bonneel,  K. Sunkavalli, J. Tompkin,  D. Sun,  S. Paris and H. Pfister.  <strong> Interactive Intrinsic Video Editing </strong>. <em>ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2014)</em>. 
          [<a href="https://perso.liris.cnrs.fr/nicolas.bonneel/intrinsicvideos.pdf">pdf</font></a>]
          [<a href="https://perso.liris.cnrs.fr/nicolas.bonneel/intrinsic.htm"><font color="#000080">code</webpage></a>]
          [<a href="https://perso.liris.cnrs.fr/nicolas.bonneel/VideoData.zip"><font color="#000080">data</font></a>]
        </td>
      </tr>   
 
    <tr>
        <td>
          <img src="./images/pubpic/ECCV2014.png" class="PaperThumbnail" width="120">
        </td>
        <td>
          L. Sevilla-Lara, D. Sun, E. Learned-Miller and MJ Black. <strong>Optical Flow Estimation with Channel Constancy </strong>. <em>European Conference on Computer Vision</em> (ECCV) 2014.  
          [<a href="https://files.is.tue.mpg.de/black/papers/ChannelConstancyECCV2014.pdf"><font color="#000080">pdf</font></a>]
        </td>
      </tr>    


      <tr>
        <td>
          <img src="./images/pubpic/CVPR14-LocalLayering.jpg" width="120">
        </td>
        <td bgcolor="#e9eaed">
          D. Sun, C. Liu and H. Pfister. <strong>Local Layering for Joint Motion Estimation and Occlusion Detection</strong>. <font face="Times New Roman"><em>IEEE Conference on Computer Vision and Pattern Recognition </em>(CVPR), 2014. </font><span class="oral">Oral presentation</span>. 
          [<a href="https://openaccess.thecvf.com/content_cvpr_2014/papers/Sun_Local_Layering_for_2014_CVPR_paper.pdf"><font color="#000080">pdf</font></a>]
        </td>
      </tr>



      <tr>
        <td>
          <img src="./images/pubpic/CVPR13.png" width="120">
        </td>
        <td>
          D. Sun, J. Wulff, E. B. Sudderth, H. Pfister  and M. J. Black.  <strong>A Fully-Connected Layered Model of Foreground and Background Flow</strong>. <font face="Times New Roman"><em>IEEE Conference on Computer Vision and Pattern Recognition </em>(CVPR), 2013. 
          [<a href="https://vcg.seas.harvard.edu/publications/a-fully-connected-layered-model-of-foreground-and-background-flow/paper"><font color="#000080">pdf</font></a>]
          [<a href="https://www.dropbox.com/s/6fa9bk7m8hux0r4/cvpr2013_code.zip"><font color="#000080">code</font></a>]
        </td>
      </tr>



      <tr>
        <td>
          <img src="./images/pubpic/TPAMI13-VSR.jpg" width="120">
        </td>
        <td bgcolor="#e9eaed">
          C. Liu and D. Sun. <strong>On Bayesian Adaptive Video Super Resolution</strong>. <span style="margin-top:4pt;margin-bottom:4pt"><font face="Times New Roman"><span style="font-size: 11pt;"><em>IEEE Transactions on Pattern Analysis and Machine Intelligence </em>(TPAMI), pp. 346-360, Vol. 36, Issue 2, 2014</span></font><span class="style16">. 
          [<a href="https://people.csail.mit.edu/celiu/pdfs/TPAMI13-VSR.pdf"><font color="#000080">pdf</font></a>]</span></span>
        </td>
      </tr>                             
            
      <tr>
        <td>
          <img src="./images/pubpic/brown_cs_logo.png" width="120">
        </td>
        <td >
          D. Sun. <strong>From Pixels to Layers: Joint Motion Estimation and Segmentation</strong>. <span style="margin-top:4pt;margin-bottom:4pt"><font face="Times New Roman"><span style="font-size: 11pt;"> Doctoral Thesis. Brown University.  May, 2013</span></font><span class="style16">. 
          [<a href="http://cs.brown.edu/~dqsun/pubs/Deqing_Sun_dissertation.pdf"><font color="#000080">pdf</font></a>]</span></span>
        </td>
      </tr>                             


      <tr>
        <td>
        <img alt="" src="./images/pubpic/ECCV12-vdb.jpg" width="120"></td>
        <td bgcolor="#e9eaed">
        <p style="margin-top:4pt;margin-bottom:4pt">
        <font face="Times New Roman">
        <span class="style16" style="font-size: 11pt;">D. Sun and C. Liu.<strong>
        Non-causal Temporal Prior for Video Deblocking.</strong></span><span style="font-size: 11pt;"><em> European Conference on Computer Vision </em>(ECCV), 20</span></font><span class="style16">12. 
        <font face="Times New Roman">[<a href="https://vcg.seas.harvard.edu/publications/non-causal-temporal-prior-for-video-deblocking/paper"><font color="#000080">pdf</font></a>]</font></span></p></td>
      </tr> 
                         
      <tr>
        <td>
          <img src="./images/pubpic/cvpr12_layer.png" width="120">
        </td>
        <td>
          D. Sun, E. B. Sudderth  and M. J. Black.  <strong>Layered Segmentation and Optical Flow Estimation Over Time</strong>. <font face="Times New Roman"><em>IEEE Conference on Computer Vision and Pattern Recognition </em>(CVPR), 2012. 
          [<a href="http://static.cs.brown.edu/people/dqsun/pubs/cvpr_2012_layer.pdf"><font color="#000080">pdf</font></a>]
        </td>
      </tr>



    <tr>
      <td>
      <img src="./images/pubpic/CVPR-VSR.jpg" width="120"></td>
      <td bgcolor="#e9eaed">
        C. Liu and D. Sun.<strong> <a style="color:black;text-decoration:none;"> A Bayesian Approach to Adaptive Video Super Resolution.</a></strong> <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (CVPR) 2011. <span class="oral">Oral presentation</span>. [<a href="https://people.csail.mit.edu/celiu/pdfs/VideoSR.pdf"><font color="#000080">pdf</font></a>][<a href="https://people.csail.mit.edu/celiu/CVPR2011/default.html"><font color="#000080">webpage</font></a>]
      </td>
    </tr>

   <tr>
      <td>
          <img src="./images/pubpic/nips2010layersimagesmall.png" class="PaperThumbnail" width="120">
        </td>
        <td>
          D. Sun, E. B. Sudderth  and M. J. Black.  <strong>Layered Image Motion with Explicit Occlusions, Temporal Consistency, and Depth Ordering</strong>. <em>Neural Information Processing Systems </em>  (NIPS), 2010. <span class="oral">Spotlight presentation. </span>  
          [<a href="http://cs.brown.edu/~dqsun/pubs/nips10paperplus.pdf">pdf</font></a>]
        </td>
      </tr>   


  <tr>
      <td>
          <img src="./images/pubpic/secrets.png" class="PaperThumbnail" width="120">
        </td>
        <td bgcolor="#e9eaed">
          D. Sun, S. Roth and M. J. Black.  <strong>Secrets of Optical Flow Estimation and Their Principles</strong>. <em>Computer Vision and Pattern Recognition </em>  (CVPR), 2010. <span class="award"> Longuet-Higgins (test-of-time) Prize at CVPR 2020. </span>  
          [<a href="http://cs.brown.edu/people/dqsun/pubs/cvpr_2010_flow.pdf">pdf</font></a>]
          [<a href="public_files/flow_code.zip">code</font></a>]
          [<a href="https://perceiving-systems.blog/en/post/the-secrets-of-the-secrets-of-optical-flow">Secrets of Secrets</font></a>]
        </td>
      </tr>   

    <tr>
      <td>
        <img src="./images/pubpic/eccv2008.png" class="PaperThumbnail" width="120">
      </td>
      <td >
        D. Sun, S. Roth,  J.P. Lewis  and M. J. Black.  <strong>Learning Optical Flow</strong>. <em> European Conference on Computer Vision </em>  (ECCV), 2008. <span class="oral"> Oral presentation. </span>  
        [<a href="http://cs.brown.edu/~dqsun/pubs/eccv2008.pdf">pdf</font></a>]
        code for [<a href="public_files/ba.zip">Black and Anandan </font></a>] [<a href="public_files/hs.zip">Horn and Schunck </font></a>]
      </td>
    </tr>   

    <tr>
      <td>
        <img src="./images/pubpic/tip07-qnoise.png" class="PaperThumbnail" width="120">
      </td>
      <td bgcolor="#e9eaed">
        D. Sun and W-K. Cham.  <strong>Postprocessing of Low Bit Rate Block DCT Coded Images based on a Fields of Experts Prior</strong>. <em> IEEE Trans. on Image Processing </em>  (TIP), 16(11), pp. 2743- 2751, Nov. 2007. 
        [<a href="http://cs.brown.edu/~dqsun/pubs/tip2007.pdf">pdf</font></a>]
        [<a href="http://cs.brown.edu/~dqsun/code/TIP07-code.zip">code</font></a>]
      </td>
    </tr>   

      <tr>
        <td>
          <img src="./images/pubpic/cuhk-ee.png" width="120">
        </td>
        <td >
          D. Sun. <strong>Postprocessing of Images Coded Using Block DCT at Low Bit Rates</strong>. <span style="margin-top:4pt;margin-bottom:4pt"><font face="Times New Roman"><span style="font-size: 11pt;"> M.Phil. Thesis. The Chinese University of Hong Kong.  July, 2007 </span></font><span class="style16">. 
          [<a href="https://core.ac.uk/download/pdf/48539121.pdf"><font color="#000080">pdf</font></a>]</span></span>
        </td>
      </tr>   

    <tr>
      <td>
        <img src="./images/pubpic/tip2007.png" class="PaperThumbnail" width="120">
      </td>
      <td bgcolor="#e9eaed">
        D. Sun and W-K. Cham.  <strong>An Effective Postprocessing Method for Lowbit Rate Block DCT Coded Images</strong>. <em> IEEE International Conference on Acoustics, Speech and Signal Processing </em>  (ICASSP), 2007. 
        [<a href="http://static.cs.brown.edu/people/dqsun/pubs/icassp2007.pdf">pdf</font></a>]
        [<a href="public_files/TIP07-code.zip">code</font></a>]
      </td>
    </tr>   


    </tbody></table>
    <p>&nbsp;</p>

     

              
    <!-- Awards session -->
    <a id="awards" class="anchor"></a><span class="section">Awards and Honors </span>

  <ul>
   
    <li>
      <p style="line-height: 150%; margin-left: 15px; margin-top: 0pt; margin-bottom: 0pt;"><b><font color="#444444" face="Arial" size="2"><a href="https://www.thecvf.com/?page_id=413#YRA"><font color="#444444" face="Arial" size="2"><b><font color="#444444" face="Arial">PAMI Young Researcher Award</font></b></font></a>, 2020</font></b></p>
    </li>

    <li>
      <p style="line-height: 150%; margin-left: 15px; margin-top: 0pt; margin-bottom: 0pt;"><b><font color="#444444" face="Arial" size="2"><a href="https://www.thecvf.com/?page_id=413#LHP"><font color="#444444" face="Arial" size="2"><b><font color="#444444" face="Arial">Longuet-Higgins (test-of-time) Prize</font></b></font></a>, 2020</font></b></p>
    </li>
           

    <li>
      <p style="line-height: 150%; margin-left: 15px; margin-top: 0pt; margin-bottom: 0pt;"><b><font color="#444444" face="Arial" size="2"><a href="http://www.robustvision.net/leaderboard.php?benchmark=flow"><font color="#444444" face="Arial" size="2"><b><font color="#444444" face="Arial">First place, Optical Flow Competition of Robust Vision Challenge</font></b></font></a>, 2020</font></b></p>
    </li>

    <li>
      <p style="line-height: 150%; margin-left: 15px; margin-top: 0pt; margin-bottom: 0pt;"><b><font color="#444444" face="Arial" size="2"><a href="https://developer.nvidia.com/blog/nvidia-splatnet-research-paper-wins-a-major-cvpr-2018-award/#:~:text=Today%20at%20the%20annual%20Computer,Sparse%20Lattice%20Networks%20for%20Point"><font color="#444444" face="Arial" size="2"><b><font color="#444444" face="Arial">CVPR Best Paper Honorable Mention</font></b></font></a>, 2018</font></b></p>
    </li>


    <li>
      <p style="line-height: 150%; margin-left: 15px; margin-top: 0pt; margin-bottom: 0pt;"><b><font color="#444444" face="Arial" size="2"><font color="#444444" face="Arial" size="2"><b><font color="#444444" face="Arial">NVAIL Pioneering Research Award (twice)</font></b></font>, 2018</font></b></p>
    </li>



    <li>
      <p style="line-height: 150%; margin-left: 15px; margin-top: 0pt; margin-bottom: 0pt;"><b><font color="#444444" face="Arial" size="2"><a href="http://www.robustvision.net/rvc2018.php"><font color="#444444" face="Arial" size="2"><b><font color="#444444" face="Arial">First place, Optical Flow Competition of Robust Vision Challenge</font></b></font></a>, 2018</font></b></p>
    </li>

    <li>
      <p style="line-height: 150%; margin-left: 15px; margin-top: 0pt; margin-bottom: 0pt;"><b><font color="#444444" face="Arial" size="2"><font color="#444444" face="Arial" size="2"><b><font color="#444444" face="Arial">Outstanding Reviewer: CVPR 2013, ACCV 2014, CVPR 2016, ECCV 2016,  CVPR 2018.</font></b></font> </font></b></p>
    </li>
    <li>
      <p style="line-height: 150%; margin-left: 15px; margin-top: 0pt; margin-bottom: 0pt;"><b><font color="#444444" face="Arial" size="2"><font color="#444444" face="Arial" size="2"><b><font color="#444444" face="Arial">Second Prize, IEEE Hong Kong Section Postgraduate Student Paper Contest</font></b></font>, 2007</font></b></p>
    </li>

  </ul>

<a id="students" class="anchor"></a><span class="section">Students </span>

<p> <font color="#444444" face="Arial" size="2"> I have been fortunate to work with many gifted students:</font> </p>
  <ul>
  <font color="#444444" face="Arial" size="2">    
   <li> <a href="https://markboss.me/">  Mark Boss </a> ( University of Tübingen), 2021  </li>   
   <li>  <a href="https://hhsinping.github.io/">Hsin-Ping Huang</a> (UC Merced), 2021  </li>   
   <li> <a href="https://www.linkedin.com/in/kyle-sargent-784006134/"> Kyle Sargent  </a> (Google AI Resident), 2021  </li>
   <li> <a href="https://scholar.google.com/citations?user=LQvi5XAAAAAJ&hl=en">   Charles Herrmann </a> (Cornell &#8594; Google), 2020-2021  </li>    
   <li> <a href="https://reagan1311.github.io/">Gen Li</a> (University of Edinburgh), 2020-2021  </li>
   <li> <a href="https://zudi-lin.github.io/"> Zudi Lin </a> ( Harvard ), 2020-2021  </li>
  <li> <a href="https://songhwanjun.github.io/"> Hwanjun Song </a> (KAIST &#8594; NAVER AI Lab), 2020-2021  </li>
   <li> <a href="https://scholar.google.com/citations?user=qsrpuKIAAAAJ&hl=en">Feitong Tan</a> (SFU), 2020  </li>   
   <li> <a href="https://gengshan-y.github.io/">Gengshan Yang</a> (CMU), 2020-2021  </li>
   <li><a href="https://rakeshjasti.github.io/">Rakesh Jasti</a> (UC Merced), 2019</li>
   <li> <a href="https://scholar.harvard.edu/jwang">  Jialiang Wang </a> (Harvard &#8594; Facebook), 2019-2020  </li>   
   <li> <a href="https://prinsphield.github.io/">Taihong Xiao </a> (UC Merced), 2019 </li>
   <li> <a href="http://www.xyang35.umiacs.io/"> Xitong Yang </a> (University of Maryland), 2019  </li>
   <li> <a href="http://graduatestudents.ucmerced.edu/wlai24/">Wei-Sheng Lai</a> (UC Merced &#8594; Google), 2018-2019  </li>
   <li> <a href="https://anuragranj.github.io/"> Anurag Ranjan   </a> (Max Planck Institute for Intelligent Systems  &#8594; Apple), 2018  </li>   
   <li> <a href="https://scholar.google.com/citations?user=DBydn38AAAAJ&hl=en"> Wei-Chih Tu </a> (National Taiwan University &#8594; Ganzin), 2017  </li>
   <li> <a href="https://scholar.google.com/citations?user=ruebFVEAAAAJ&hl=en&oi=ao">  Jiangxin Dong</a> ( DUST &#8594; Max Planck Institute for Informatics), 2017-2018 </li>
   <li> <a href="https://jianghz.me/">  Huaizu Jiang </a> ( UMass Amherst &#8594; Northeastern), 2017-2019  </li>
   <li> <a href="https://lvzhaoyang.github.io/">   Zhaoyang Lv  </a> (Georgia Tech &#8594; Facebook), 2017, 2019  </li> 
   <li> <a href="https://suhangpro.github.io/">  Hang Su </a> ( UMass Amherst &#8594; NVIDIA Research), 2017-2018  </li>
   <li> <a href="https://sites.google.com/site/yihsuantsai/">Yi-Hsuan Tsai </a>(UC Merced &#8594; NEC Lab &#8594; Phiar ), 2017  </li>
  <li> <a href="https://jrenzhile.com/">Zhile Ren</a> (Brown &#8594; Georgia Tech &#8594; Apple), 2016-2017  </li>
  <li><a href="https://sites.google.com/corp/view/xiangyuxu">Xiangyu Xu</a> (Tsinghua/UC Merced &#8594; CMU/MIT &#8594; NTU), 2016-2017 </li>
  <li> <a href="https://jspan.github.io/">Jinshan Pan</a> (DUST/UC Merced &#8594;  NUST), 2015  </li>
<!-- <li> <a href=" ">  </a> ( &#8594; )  </li> -->
  </font>
  </ul>

  <p><font color="#444444" face="Arial" size="2">&copy 2021 Deqing Sun. Thanks <a href="http://people.csail.mit.edu/celiu"><font color="#000080">Dr. Ce Liu</font></a> for the template. </font></p>

  </div>
  <script>
    var thumbnails = document.getElementsByClassName("PaperThumbnail");
    var i;
    for (i = 0; i < thumbnails.length; i++) {
      thumbnails[i].width = "120"
    }
  </script>  


</body></html>